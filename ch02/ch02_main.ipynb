{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8045689a",
   "metadata": {},
   "source": [
    "## Chapter 2: Generating Text with a Pre-trained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f3265d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning_from_scratch version: 0.1.13\n",
      "torch version: 2.10.0\n",
      "tokenizers version: 0.22.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "used_libraries = [\n",
    "    \"reasoning_from_scratch\",\n",
    "    \"torch\",\n",
    "    \"tokenizers\"  # Used by reasoning_from_scratch\n",
    "]\n",
    "\n",
    "for lib in used_libraries:\n",
    "    print(f\"{lib} version: {version(lib)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc58283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.10.0\n",
      "Apple Silicon GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA GPU\")\n",
    "elif torch.mps.is_available():\n",
    "    print(\"Apple Silicon GPU\")\n",
    "elif torch.xpu.is_available():\n",
    "    print(\"Intel GPU\")\n",
    "else:\n",
    "    print(\"Only CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c184a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reasoning_from_scratch.qwen3 import download_qwen3_small\n",
    "\n",
    "download_qwen3_small(kind=\"base\", tokenizer_only=True, out_dir=\"qwen3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55bc6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from reasoning_from_scratch.qwen3 import Qwen3Tokenizer\n",
    "\n",
    "tokenizer_path = Path(\"qwen3\") / \"tokenizer-base.json\"\n",
    "tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1876a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain large language models.\"\n",
    "input_token_ids_list = tokenizer.encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ccab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840 --> Ex\n",
      "20772 --> plain\n",
      "3460 -->  large\n",
      "4128 -->  language\n",
      "4119 -->  models\n",
      "13 --> .\n"
     ]
    }
   ],
   "source": [
    "for i in input_token_ids_list:\n",
    "    print(f\"{i} --> {tokenizer.decode([i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77522d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain large language models.\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(input_token_ids_list)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07107cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "def get_device(enable_tensor_cores=True):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using NVIDIA CUDA GPU\")\n",
    "        \n",
    "        if enable_tensor_cores:\n",
    "            major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "            if (major, minor) >= (2, 9):\n",
    "                torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "                torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
    "            else:\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple Silicon GPU (MPS)\")\n",
    "\n",
    "    elif torch.xpu.is_available():\n",
    "        device = torch.device(\"xpu\")\n",
    "        print(\"Using Intel GPU\")\n",
    "\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9c30a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f73f757a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen3-0.6B-base.pth: 100% (1433 MiB / 1433 MiB)\n"
     ]
    }
   ],
   "source": [
    "download_qwen3_small(kind=\"base\", tokenizer_only=False, out_dir=\"qwen3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1db7279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (tok_emb): Embedding(151936, 1024)\n",
       "  (trf_blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (att): GroupedQueryAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc2): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc3): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reasoning_from_scratch.qwen3 import Qwen3Model, QWEN_CONFIG_06_B\n",
    "\n",
    "model_path = Path(\"qwen3\") / \"qwen3-0.6B-base.pth\"\n",
    "\n",
    "model = Qwen3Model(QWEN_CONFIG_06_B)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8706faea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "example = torch.tensor([1, 2,3 ])\n",
    "print(example)\n",
    "print(example.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe02ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "example = torch.tensor([[1, 2, 3]])\n",
    "print(example)\n",
    "print(example.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61a877b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input tokens: 6\n",
      "Formatted Output tensor shape: torch.Size([6, 151936])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain large language models.\"\n",
    "input_token_ids_list = tokenizer.encode(prompt)\n",
    "print(f\"Number of input tokens: {len(input_token_ids_list)}\")\n",
    "\n",
    "input_tensor = torch.tensor(input_token_ids_list)\n",
    "input_tensor_fmt = input_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "output_tensor = model(input_tensor_fmt)\n",
    "output_tensor_fmt = output_tensor.squeeze(0)\n",
    "print(f\"Formatted Output tensor shape: {output_tensor_fmt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6735b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.3750,  2.0312,  8.0000,  ..., -2.5469, -2.5469, -2.5469],\n",
      "       dtype=torch.bfloat16)\n",
      "torch.Size([151936])\n"
     ]
    }
   ],
   "source": [
    "last_token = output_tensor_fmt[-1].detach()\n",
    "print(last_token)\n",
    "print(last_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "615f68d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20286])\n"
     ]
    }
   ],
   "source": [
    "print(last_token.argmax(dim=-1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "315c9c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([20286]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07e919d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "example = torch.tensor([-2, 1, 3, 1])\n",
    "print(torch.max(example))\n",
    "print(torch.argmax(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cadf604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_basic(\n",
    "    model,\n",
    "    token_ids,\n",
    "    max_new_tokens,\n",
    "    eos_token_id=None\n",
    "):\n",
    "    input_length = token_ids.shape[1]\n",
    "    model.eval()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        out = model(token_ids)[:, -1]\n",
    "        next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    "\n",
    "        if (eos_token_id is not None\n",
    "                and next_token.item() == eos_token_id):\n",
    "            break\n",
    "        # print(\"token_ids: \", token_ids.shape)\n",
    "        # print(\"next_token: \", next_token.shape)\n",
    "        token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "        # print(\"new_tokenids: \", token_ids.shape)\n",
    "        \n",
    "    return token_ids[:, input_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "115c1088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.<|endoftext|>Human language is a complex and dynamic system that has evolved over millions of years to enable effective communication and social interaction. It is composed of a vast array of symbols, including letters, numbers, and words, which are used to convey meaning and express thoughts and ideas. The evolution of language has\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain large language models in a single sentence.\"\n",
    "\n",
    "input_token_ids_tensor = torch.tensor(\n",
    "    tokenizer.encode(prompt),\n",
    "    device=device).unsqueeze(0)\n",
    "print(input_token_ids_tensor.shape)\n",
    "max_new_tokens = 100\n",
    "\n",
    "output_token_ids_tensor = generate_text_basic(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens\n",
    ")\n",
    "output_text = tokenizer.decode(\n",
    "    output_token_ids_tensor.squeeze(0).tolist()\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6683f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151643]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"<|endoftext|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31a70bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151643\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fb5f3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n"
     ]
    }
   ],
   "source": [
    "output_token_ids_tensor = generate_text_basic(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "output_text = tokenizer.decode(\n",
    "    output_token_ids_tensor.squeeze(0).tolist()\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd3f664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats(output_token_ids, tokenizer, start_time,\n",
    "                   end_time, print_tokens=True):\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Time: {total_time:.2f} sec\")\n",
    "    print(f\"{int(output_token_ids.numel() / total_time)} tokens/sec\")\n",
    "\n",
    "    for name, backend in ((\"CUDA\", getattr(torch, \"cuda\", None)),\n",
    "                          (\"XPU\", getattr(torch, \"xpu\", None))):\n",
    "        if backend is not None and backend.is_available():\n",
    "            max_mem_bytes = backend.max_memory_allocated()\n",
    "            max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "            print(f\"Max {name} memory allocated: {max_mem_gb:.2f} GB\")\n",
    "            backend.reset_peak_memory_stats()\n",
    "\n",
    "    if print_tokens:\n",
    "        output_text = tokenizer.decode(output_token_ids.squeeze(0).tolist())\n",
    "        print(f\"\\n{output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "768a79f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13.86 sec\n",
      "2 tokens/sec\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "output_token_ids_tensor = generate_text_basic(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "end_time = time.time()\n",
    "generate_stats(output_token_ids_tensor, tokenizer, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1594b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reasoning_from_scratch.qwen3 import KVCache\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_text_basic_cache(\n",
    "    model,\n",
    "    token_ids,\n",
    "    max_new_tokens,\n",
    "    eos_token_id=None\n",
    "):\n",
    "    input_length = token_ids.shape[1]\n",
    "    model.eval()\n",
    "    cache=KVCache(n_layers=model.cfg[\"n_layers\"])\n",
    "    model.reset_kv_cache()\n",
    "\n",
    "    out = model(token_ids, cache=cache)[:, -1]\n",
    "    for _ in range(max_new_tokens):\n",
    "        next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    "\n",
    "        if (eos_token_id is not None\n",
    "                and next_token.item() == eos_token_id):\n",
    "            break\n",
    "        token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "        out = model(next_token, cache)[:, -1]\n",
    "    return token_ids[:, input_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7bc5ccf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.31 sec\n",
      "31 tokens/sec\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "output_token_ids_tensor = generate_text_basic_cache(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "generate_stats(output_token_ids_tensor, tokenizer, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35a6475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "if (major, minor) >= (2, 8):\n",
    "    # This avoids retriggering model recompilations \n",
    "    # in PyTorch 2.8 and newer\n",
    "    # if the model contains code like self.pos = self.pos + 1\n",
    "    torch._dynamo.config.allow_unspec_int_on_nn_module = True\n",
    "\n",
    "model_compiled = torch.compile(model)\n",
    "\n",
    "# If you have issues with torch.compile on \"mps\" devices and get an InductorError,\n",
    "# make sure you are using PyTorch 2.9 or newer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea115f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up run\n",
      "Time: 61.24 sec\n",
      "0 tokens/sec\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Timed run1:\n",
      "Time: 12.96 sec\n",
      "3 tokens/sec\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Timed run2:\n",
      "Time: 13.78 sec\n",
      "2 tokens/sec\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    start_time=time.time()\n",
    "    output_token_ids_tensor = generate_text_basic(\n",
    "        model=model_compiled,\n",
    "        token_ids=input_token_ids_tensor,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if i == 0:\n",
    "        print(\"Warm-up run\")\n",
    "    else:\n",
    "        print(f\"Timed run{i}:\")\n",
    "    generate_stats(output_token_ids_tensor, tokenizer, start_time, end_time)\n",
    "\n",
    "    print(f\"\\n{30*'-'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "811da972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up run\n",
      "Time: 56.01 sec\n",
      "0 tokens/sec\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Timed run1:\n",
      "Time: 1.02 sec\n",
      "40 tokens/sec\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Timed run2:\n",
      "Time: 1.04 sec\n",
      "39 tokens/sec\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    start_time=time.time()\n",
    "    output_token_ids_tensor = generate_text_basic_cache(\n",
    "        model=model_compiled,\n",
    "        token_ids=input_token_ids_tensor,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if i == 0:\n",
    "        print(\"Warm-up run\")\n",
    "    else:\n",
    "        print(f\"Timed run{i}:\")\n",
    "    generate_stats(output_token_ids_tensor, tokenizer, start_time, end_time)\n",
    "\n",
    "    print(f\"\\n{30*'-'}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b6957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
